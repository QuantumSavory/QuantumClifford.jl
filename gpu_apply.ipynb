{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88702c5-d2f0-4f1e-a181-9f45c4419f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using BenchmarkTools\n",
    "using QuantumClifford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7212e-6fd1-4eaf-9c6d-f8b1b8bace34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to determine what devices we are running on and attempt to determine comparability of hardware.\n",
    "CUDA.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40844bc-ea8d-42a3-8db1-ad39637a2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sys.cpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb14b6a-b3cc-4df0-b63e-811c9804992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original GPU apply function, for qubits<=64\n",
    "function apply_gate_gpu!(s::CuArray{UInt64, 2}, c::CuArray{UInt64, 2})\n",
    "    # Kernel function\n",
    "    function kernel(s, c, num_bits)\n",
    "        i = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n",
    "        if i <= num_bits\n",
    "            tempX = UInt64(0)\n",
    "            tempZ = UInt64(0)\n",
    "            tempSX = s[1, i]\n",
    "            tempSZ = s[2, i]\n",
    "            #@cuprintln(\"tempSX %d\\n\", tempSX)\n",
    "            #@cuprintln(\"tempSZ %d\\n\", tempSZ)\n",
    "            for j in 1:num_bits\n",
    "                if tempSX & 1 == 1\n",
    "                    #=\n",
    "                    @cuprintf(\"Here X %d\\n\", i)\n",
    "                    @cuprintln(j)\n",
    "                    @cuprintln(c[1,j])\n",
    "                    @cuprintf(\"After here X%d\\n\", i)\n",
    "                    =#\n",
    "                    tempX ⊻= c[1, j]\n",
    "                    #hard coded for 4 qubits for now\n",
    "                    tempZ ⊻= c[2, j]\n",
    "                    #@cuprintln(\"tempX %d\\n\", tempX)\n",
    "                    #@cuprintln(\"tempZ %d\\n\", tempZ)\n",
    "                end\n",
    "                if tempSZ & 1 == 1\n",
    "                    #=\n",
    "                    @cuprintf(\"Here Z %d\\n\", i)\n",
    "                    @cuprintln(j)\n",
    "                    @cuprintln(c[2,j])\n",
    "                    @cuprintf(\"After here Z%d\\n\", i)\n",
    "                    =#\n",
    "                    tempX ⊻= c[1, j+num_bits]\n",
    "                    #hard coded for 4 qubits for now\n",
    "                    tempZ ⊻= c[2, j+num_bits]\n",
    "                    #@cuprintln(\"tempX %d\\n\", tempX)\n",
    "                    #@cuprintln(\"tempZ %d\\n\", tempZ)\n",
    "                end\n",
    "                tempSX >>= 1\n",
    "                tempSZ >>= 1\n",
    "            end\n",
    "            s[1, i] = tempX\n",
    "            s[2, i] = tempZ\n",
    "        end\n",
    "        return\n",
    "    end\n",
    "\n",
    "    # Check if input exceeds 64 bits\n",
    "    if size(s, 2) > 64\n",
    "        println(\"MORE THAN 64\")\n",
    "        return\n",
    "    end\n",
    "\n",
    "    # Launch the kernel\n",
    "    num_bits = size(s, 2)\n",
    "    #println(num_bits)\n",
    "    threads_per_block = 64\n",
    "    blocks = ceil(Int, num_bits / threads_per_block)\n",
    "    @cuda threads=threads_per_block blocks=blocks kernel(s, c, num_bits)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ca785-4927-4afa-9524-f57e4674b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing apply_gate_gpu!\n",
    "num_qbits = 64\n",
    "#can change num_qbits to any number between 1 and 64 inclusive\n",
    "cliff = random_clifford(num_qbits)\n",
    "#The below comments are useful for visualizing how the data is actually stored\n",
    "#println(cliff.tab.xzs)\n",
    "#println(map(bitstring, cliff.tab.xzs))\n",
    "#println(map(bitstring, cliff.tab.xzs[1, :]))\n",
    "#println(map(bitstring, cliff.tab.xzs[1,3]))\n",
    "#dump(cliff) VERY useful for finding functions\n",
    "c_gpu = CuArray(cliff.tab.xzs)  # Copy `c` to GPU memory once\n",
    "s = random_stabilizer(num_qbits)\n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "#println(s.tab.xzs)\n",
    "\n",
    "correct = apply!(s, cliff, phases=false)\n",
    "correct = correct.tab.xzs\n",
    "println(\"Correct answer: $correct\")\n",
    "apply_gate_gpu!(s_gpu, c_gpu)\n",
    "println(s_gpu)\n",
    "gpu_result = Array(s_gpu)\n",
    "# Verify correctness\n",
    "@assert correct == gpu_result \"GPU and CPU results do not match!\"\n",
    "println(\"Verification successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b681dec-4fe3-487b-b354-6bde7c56e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchmarking apply_gate_gpu!\n",
    "num_qbits = 64\n",
    "#can change num_qbits to anything from 1-64\n",
    "cliff = random_clifford(num_qbits)\n",
    "c_gpu = CuArray(cliff.tab.xzs)  # Copy `c` to GPU memory once\n",
    "s = random_stabilizer(num_qbits)\n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "tempS = tab(s).xzs\n",
    "tempCliff = cliff.tab.xzs\n",
    "println(\"Benchmarking GPU\")\n",
    "#DON'T FORGET CUDA.@sync, otherwise will just benchmark the time it takes to transfer over data\n",
    "gpu_time = @benchmark CUDA.@sync apply_gate_gpu!(s_gpu, c_gpu)\n",
    "println(\"GPU\")\n",
    "println(gpu_time)\n",
    "show(stdout, \"text/plain\", gpu_time)\n",
    "# CPU Benchmark\n",
    "println()\n",
    "println(\"Benchmarking CPU\")\n",
    "cpu_time = @benchmark apply!(s, cliff, phases=false)\n",
    "println(\"CPU\")\n",
    "println(cpu_time)\n",
    "show(stdout, \"text/plain\", cpu_time)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695781c1-f020-4695-a90b-92cfc4178330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU again for 65+ qubits (really 1-infinite qubits)\n",
    "\n",
    "#Profiling tools (NVIDIA) could make speed-ups, help determine what the bottleneck is. More for educational value and something I didn't have time to look into\n",
    "function apply_gate_gpu_64plus!(s, c, num_bits)\n",
    "    #z_start is where the z's start for example if there are 67 bits it will be 3, if there are 129 bits it will be 4 etc.\n",
    "    #x_stop is just 1 before z_start\n",
    "    function kernel(s, c, num_bits, numBitstrings, tempXZArr, x_stop, z_start)\n",
    "        i = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n",
    "        if i <= num_bits\n",
    "            for j in 1:64\n",
    "                #x's loop\n",
    "                for k in 1:x_stop\n",
    "                    if s[k, i] & 1 == 1\n",
    "                        for l in 1:numBitstrings\n",
    "                            #before it was just c[l, j], but now a bit more complicated indexing\n",
    "                            tempXZArr[i, l] ⊻= c[l, j+(k-1)*64]\n",
    "                        end\n",
    "                    end\n",
    "                    s[k, i] >>= 1\n",
    "                end\n",
    "                #z's loop\n",
    "                for k in z_start:numBitstrings\n",
    "                    if s[k, i] & 1 == 1\n",
    "                        for l in 1:numBitstrings\n",
    "                             #before it was just c[l, j+ num_bits], but now a bit more complicated\n",
    "                            tempXZArr[i, l] ⊻= c[l, j+num_bits+(k-z_start)*64]\n",
    "                        end\n",
    "                    end\n",
    "                    s[k, i] >>= 1\n",
    "                end\n",
    "            end\n",
    "            for a in 1:numBitstrings\n",
    "                s[a, i] = tempXZArr[i, a]\n",
    "            end\n",
    "        end\n",
    "        return \n",
    "    end\n",
    "\n",
    "\n",
    "    # Launch the kernel\n",
    "    #currently we are passed in number of bits, which I don't think is unreasonable, although could be done away with, with some calculations)\n",
    "    numBitstrings = ceil(Int, num_bits/64)*2\n",
    "    x_stop = Int(numBitstrings / 2)\n",
    "    z_start = Int((numBitstrings / 2) + 1)\n",
    "    tempXZArr = CUDA.fill(UInt64(0), num_bits, numBitstrings)\n",
    "    #starting XZArr at 0's\n",
    "    threads_per_block = 64\n",
    "    blocks = ceil(Int, num_bits / threads_per_block)\n",
    "    @cuda threads=threads_per_block blocks=blocks kernel(s, c, num_bits, numBitstrings, tempXZArr, x_stop, z_start)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2ced4-8781-45f7-8186-ca7f1a6b6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing apply_gate_gpu_64plus!\n",
    "num_qbits = 20000\n",
    "#tensor_pow option used if we want to test a larger state/gate since random_clifford takes a long time as input gets large >2000\n",
    "#Otherwise can use the below two lines\n",
    "#cliff = random_clifford(num_qbits)\n",
    "#s = random_stabilizer(num_qbits)\n",
    "#If using tensor_pow need to adjust values accordingly, i.e. for 10,000 qubits do...\n",
    "#num_qbits = 10000, \n",
    "#cliff = tensor_pow(random_clifford(1000),10) or tensor_pow(random_clifford(500),20) \n",
    "#s = s = tensor_pow(random_stabilizer(1000),10)\n",
    "cliff = tensor_pow(random_clifford(1000),20)\n",
    "c_gpu = CuArray(cliff.tab.xzs)  # Copy `c` to GPU memory once\n",
    "s = tensor_pow(random_stabilizer(1000),20)\n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "\n",
    "\n",
    "correct = apply!(s, cliff, phases=false)\n",
    "correct = correct.tab.xzs\n",
    "#println(\"Correct answer: $correct\")\n",
    "apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "gpu_result = Array(s_gpu)\n",
    "# Verify correctness\n",
    "@assert correct == gpu_result \"GPU and CPU results do not match!\"\n",
    "println(\"Verification successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a29b1-b013-470a-bdbe-8e5238457006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchmarking apply_gate_gpu_64plus!\n",
    "num_qbits = 1024\n",
    "cliff = random_clifford(num_qbits) #have same tensor_pow option as previous cell\n",
    "c_gpu = CuArray(cliff.tab.xzs)  # Copy `c` to GPU memory once\n",
    "s = random_stabilizer(num_qbits)\n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "tempS = tab(s).xzs\n",
    "tempCliff = cliff.tab.xzs\n",
    "correct = apply!(s, cliff, phases=false)\n",
    "correct = correct.tab.xzs\n",
    "# GPU Benchmark\n",
    "println(\"Benchmarking GPU\")\n",
    "#DON'T FORGET CUDA.@sync \n",
    "gpu_time = @benchmark CUDA.@sync apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "println(\"GPU\")\n",
    "println(gpu_time)\n",
    "show(stdout, \"text/plain\", gpu_time)\n",
    "# CPU Benchmark\n",
    "println(\"Benchmarking CPU\")\n",
    "cpu_time = @benchmark apply!(s, cliff, phases=false)\n",
    "println(\"CPU\")\n",
    "println(cpu_time)\n",
    "show(stdout, \"text/plain\", cpu_time)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5abfe6-62da-42a0-8dd7-da0ff4cf769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to get an idea of what exactly is occupying the most time, not very successful but keeping here in case others can parse it better than I\n",
    "using Profile\n",
    "num_qbits = 20000\n",
    "#cliff = random_clifford(num_qbits)\n",
    "cliff = tensor_pow(random_clifford(1000),20)\n",
    "c_gpu = CuArray(cliff.tab.xzs)  # Copy `c` to GPU memory once\n",
    "#s = random_stabilizer(num_qbits)\n",
    "s = tensor_pow(random_stabilizer(1000),20)\n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "@profile apply!(s, cliff, phases=false)\n",
    "Profile.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a85d3-4645-400f-942b-b09af8d2b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qbits = 20000\n",
    "#cliff = random_clifford(num_qbits)\n",
    "cliff = tensor_pow(random_clifford(1000),20)\n",
    "c_gpu = CuArray(cliff.tab.xzs)  # Copy `c` to GPU memory once\n",
    "#s = random_stabilizer(num_qbits)\n",
    "s = tensor_pow(random_stabilizer(1000),20)\n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "tempS = tab(s).xzs\n",
    "tempCliff = cliff.tab.xzs\n",
    "correct = apply!(s, cliff, phases=false)\n",
    "correct = correct.tab.xzs\n",
    "#println(\"Correct answer: $correct\")\n",
    "# GPU Benchmark\n",
    "println(\"Benchmarking GPU\")\n",
    "#DON'T FORGET CUDA.@sync \n",
    "gpu_time = @benchmark CUDA.@sync apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "println(\"GPU\")\n",
    "println(gpu_time)\n",
    "show(stdout, \"text/plain\", gpu_time)\n",
    "# CPU Benchmark\n",
    "println(\"Benchmarking CPU\")\n",
    "cpu_time = @benchmark apply!(s, cliff, phases=false)\n",
    "println(\"CPU\")\n",
    "println(cpu_time)\n",
    "show(stdout, \"text/plain\", cpu_time)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd165a3-df12-4497-b083-9acf1f9c659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CPU is single-threaded\n",
    "#it should be noted we select 2 threads here because we are running on the Unity cluster. \n",
    "#It is advised, prior to startup, that at least 2 CPU threads are used so one can focus on simply running the webapplication. \n",
    "#Thus we select 2 threads, also when I tried to run with only 1 it would fail, so that extra thread seems to be focused so\n",
    "#This for loop is giving issues, it worked once but then threw errors from then on, not entirely sure why so just leaving it here as it is nicer than listing everything out if the issue can be resolved\n",
    "#Still not entirely sure why it doesn't work sometimes, but as of now if I define c_gpu and s_gpu somewhere previously and run that codeblock this seems to run but keep that s_gpu and c_gpu for the whole time\n",
    "#=\n",
    "using LinearAlgebra\n",
    "#BLAS.set_num_threads(2)\n",
    "\n",
    "# Define problem sizes\n",
    "problem_sizes = [64, 512, 1024, 2000, 5000, 10000, 20000]\n",
    "cpu_times = Float64[]\n",
    "gpu_times = Float64[]\n",
    "\n",
    "for num_qbits in problem_sizes\n",
    "    println(\"\\nBenchmarking with num_qbits = $num_qbits\")\n",
    "    if num_qbits >= 2000\n",
    "        cliff = tensor_pow(random_clifford(1000),num_qbits/1000)\n",
    "        s = tensor_pow(random_stabilizer(1000),num_qbits/1000)\n",
    "    else\n",
    "        cliff = random_clifford(num_qbits)\n",
    "        s = random_stabilizer(num_qbits)\n",
    "    end\n",
    "    c_gpu = CuArray(cliff.tab.xzs)  \n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    \n",
    "\n",
    "    # CPU Benchmark\n",
    "    cpu_bench = @benchmark apply!(s, cliff, phases=false)\n",
    "    cpu_time = median(cpu_bench.times) / 1e9  # Convert to seconds\n",
    "    println(\"CPU time: $cpu_time seconds\")\n",
    "    push!(cpu_times, cpu_time)\n",
    "\n",
    "    # GPU Benchmark\n",
    "    gpu_bench = @benchmark CUDA.@sync apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "    gpu_time = median(gpu_bench.times) / 1e9  # Convert to seconds\n",
    "    println(\"GPU time: $gpu_time seconds\")\n",
    "    push!(gpu_times, gpu_time)\n",
    "end\n",
    "\n",
    "# Plot results (log-log scale)\n",
    "plot(problem_sizes, cpu_times, label=\"CPU\", linewidth=2, marker=:circle, xscale=:log10, yscale=:log10)\n",
    "plot!(problem_sizes, gpu_times, label=\"GPU\", linewidth=2, marker=:square)\n",
    "xlabel!(\"Problem Size (num_qbits)\")\n",
    "ylabel!(\"Execution Time (s)\")\n",
    "title!(\"CPU vs GPU Performance\")\n",
    "savefig(\"benchmark_plot.png\")\n",
    "=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc78f20-a45c-42cd-8aae-94d496950de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CPU is single-threaded\n",
    "#it should be noted we select 2 threads here because we are running on the Unity cluster. \n",
    "#It is advised, prior to startup, that at least 2 CPU threads are used so one can focus on simply running the webapplication. \n",
    "#Thus we select 2 threads, also when I tried to run with only 1 it would fail, so that extra thread seems to be focused so\n",
    "BLAS.set_num_threads(2)\n",
    "\n",
    "# Define problem sizes (make sure these match the sizes we actually use)\n",
    "problem_sizes = [64, 512, 1024, 2000, 5000,10000, 20000]\n",
    "cpu_times = Float64[]\n",
    "gpu_times = Float64[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6fb33-f1dc-417b-a486-8a4bf8c595c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#64 qubit benchmark\n",
    "num_qbits=64\n",
    "println(\"\\nBenchmarking with num_qbits = $num_qbits\")\n",
    "\n",
    "cliff = random_clifford(num_qbits)\n",
    "s = random_stabilizer(num_qbits)\n",
    "\n",
    "c_gpu = CuArray(cliff.tab.xzs)  \n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "\n",
    "# GPU Benchmark\n",
    "gpu_bench = @benchmark CUDA.@sync apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "gpu_time = median(gpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"GPU time: $gpu_time seconds\")\n",
    "push!(gpu_times, gpu_time)\n",
    "show(stdout, \"text/plain\", gpu_bench)\n",
    "println()\n",
    "# CPU Benchmark\n",
    "cpu_bench = @benchmark apply!(s, cliff, phases=false)\n",
    "cpu_time = median(cpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"CPU time: $cpu_time seconds\")\n",
    "push!(cpu_times, cpu_time)\n",
    "show(stdout, \"text/plain\", cpu_bench)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c85a9-1ab6-43e6-b8fc-ef8a652e6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#512 qubit benchmark\n",
    "num_qbits=512\n",
    "println(\"\\nBenchmarking with num_qbits = $num_qbits\")\n",
    "\n",
    "cliff = random_clifford(num_qbits)\n",
    "s = random_stabilizer(num_qbits)\n",
    "\n",
    "c_gpu = CuArray(cliff.tab.xzs)  \n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "\n",
    "# GPU Benchmark\n",
    "gpu_bench = @benchmark CUDA.@sync apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "gpu_time = median(gpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"GPU time: $gpu_time seconds\")\n",
    "push!(gpu_times, gpu_time)\n",
    "show(stdout, \"text/plain\", gpu_bench)\n",
    "println()\n",
    "# CPU Benchmark\n",
    "cpu_bench = @benchmark apply!(s, cliff, phases=false)\n",
    "cpu_time = median(cpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"CPU time: $cpu_time seconds\")\n",
    "push!(cpu_times, cpu_time)\n",
    "show(stdout, \"text/plain\", cpu_bench)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f80925-45e7-4b4f-8d70-3a97b956591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1024 qubit benchmark\n",
    "num_qbits=1024\n",
    "println(\"\\nBenchmarking with num_qbits = $num_qbits\")\n",
    "\n",
    "cliff = random_clifford(num_qbits)\n",
    "s = random_stabilizer(num_qbits)\n",
    "\n",
    "c_gpu = CuArray(cliff.tab.xzs)  \n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "\n",
    "# GPU Benchmark\n",
    "gpu_bench = @benchmark CUDA.@sync apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "gpu_time = median(gpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"GPU time: $gpu_time seconds\")\n",
    "push!(gpu_times, gpu_time)\n",
    "show(stdout, \"text/plain\", gpu_bench)\n",
    "println()\n",
    "# CPU Benchmark\n",
    "cpu_bench = @benchmark apply!(s, cliff, phases=false)\n",
    "cpu_time = median(cpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"CPU time: $cpu_time seconds\")\n",
    "push!(cpu_times, cpu_time)\n",
    "show(stdout, \"text/plain\", cpu_bench)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d96dd-fc8f-4776-911a-14cd26844c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2000 qubit benchmark\n",
    "num_qbits=2000\n",
    "println(\"\\nBenchmarking with num_qbits = $num_qbits\")\n",
    "\n",
    "cliff = tensor_pow(random_clifford(1000),num_qbits/1000)\n",
    "s = tensor_pow(random_stabilizer(1000),num_qbits/1000)\n",
    "c_gpu = CuArray(cliff.tab.xzs)  \n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "\n",
    "# GPU Benchmark\n",
    "gpu_bench = @benchmark CUDA.@sync apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "gpu_time = median(gpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"GPU time: $gpu_time seconds\")\n",
    "push!(gpu_times, gpu_time)\n",
    "show(stdout, \"text/plain\", gpu_bench)\n",
    "println()\n",
    "# CPU Benchmark\n",
    "cpu_bench = @benchmark apply!(s, cliff, phases=false)\n",
    "cpu_time = median(cpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"CPU time: $cpu_time seconds\")\n",
    "push!(cpu_times, cpu_time)\n",
    "show(stdout, \"text/plain\", cpu_bench)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0555b-fcb1-42d4-9e47-1f7a27436066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000 qubit benchmark\n",
    "num_qbits=5000\n",
    "println(\"\\nBenchmarking with num_qbits = $num_qbits\")\n",
    "\n",
    "cliff = tensor_pow(random_clifford(1000),num_qbits/1000)\n",
    "s = tensor_pow(random_stabilizer(1000),num_qbits/1000)\n",
    "c_gpu = CuArray(cliff.tab.xzs)  \n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "\n",
    "# GPU Benchmark\n",
    "gpu_bench = @benchmark CUDA.@sync apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "gpu_time = median(gpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"GPU time: $gpu_time seconds\")\n",
    "push!(gpu_times, gpu_time)\n",
    "show(stdout, \"text/plain\", gpu_bench)\n",
    "println()\n",
    "# CPU Benchmark\n",
    "cpu_bench = @benchmark apply!(s, cliff, phases=false)\n",
    "cpu_time = median(cpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"CPU time: $cpu_time seconds\")\n",
    "push!(cpu_times, cpu_time)\n",
    "show(stdout, \"text/plain\", cpu_bench)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed594f-f833-49ed-aa32-74611e18734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10000 qubit benchmark\n",
    "num_qbits=10000\n",
    "println(\"\\nBenchmarking with num_qbits = $num_qbits\")\n",
    "\n",
    "cliff = tensor_pow(random_clifford(1000),num_qbits/1000)\n",
    "s = tensor_pow(random_stabilizer(1000),num_qbits/1000)\n",
    "c_gpu = CuArray(cliff.tab.xzs)  \n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "\n",
    "# GPU Benchmark\n",
    "gpu_bench = @benchmark CUDA.@sync apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "gpu_time = median(gpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"GPU time: $gpu_time seconds\")\n",
    "push!(gpu_times, gpu_time)\n",
    "show(stdout, \"text/plain\", gpu_bench)\n",
    "println()\n",
    "# CPU Benchmark\n",
    "cpu_bench = @benchmark apply!(s, cliff, phases=false)\n",
    "cpu_time = median(cpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"CPU time: $cpu_time seconds\")\n",
    "push!(cpu_times, cpu_time)\n",
    "show(stdout, \"text/plain\", cpu_bench)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a8dce-b231-4db9-9d2a-849a5d2f7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20000 qubit benchmark\n",
    "num_qbits=20000\n",
    "println(\"\\nBenchmarking with num_qbits = $num_qbits\")\n",
    "\n",
    "cliff = tensor_pow(random_clifford(1000),num_qbits/1000)\n",
    "s = tensor_pow(random_stabilizer(1000),num_qbits/1000)\n",
    "c_gpu = CuArray(cliff.tab.xzs)  \n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "\n",
    "# GPU Benchmark\n",
    "gpu_bench = @benchmark CUDA.@sync apply_gate_gpu_64plus!(s_gpu, c_gpu, num_qbits)\n",
    "gpu_time = median(gpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"GPU time: $gpu_time seconds\")\n",
    "push!(gpu_times, gpu_time)\n",
    "show(stdout, \"text/plain\", gpu_bench)\n",
    "println()\n",
    "# CPU Benchmark\n",
    "cpu_bench = @benchmark apply!(s, cliff, phases=false)\n",
    "cpu_time = median(cpu_bench.times) / 1e9  # Convert to seconds\n",
    "println(\"CPU time: $cpu_time seconds\")\n",
    "push!(cpu_times, cpu_time)\n",
    "show(stdout, \"text/plain\", cpu_bench)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01462dd-0eee-40f8-9a6e-c64150f23e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "plot(problem_sizes, cpu_times, label=\"CPU\", linewidth=2, marker=:circle, xscale=:log10, yscale=:log10)\n",
    "plot!(problem_sizes, gpu_times, label=\"GPU\", linewidth=2, marker=:square)\n",
    "xlabel!(\"Problem Size (num_qbits)\")\n",
    "ylabel!(\"Execution Time (s)\")\n",
    "title!(\"CPU vs GPU Performance\")\n",
    "savefig(\"benchmark_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55615ce-831c-4d94-8f95-4a217b0f281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_sizes = [64, 512, 1024, 2000, 5000,20000]\n",
    "# Plot results (log-log scale)\n",
    "plot(problem_sizes, cpu_times, label=\"CPU\", linewidth=2, marker=:circle, xscale=:log10, yscale=:log10)\n",
    "plot!(problem_sizes, gpu_times, label=\"GPU\", linewidth=2, marker=:square)\n",
    "xlabel!(\"Problem Size (num_qbits)\")\n",
    "ylabel!(\"Execution Time (s)\")\n",
    "title!(\"CPU vs GPU Performance\")\n",
    "savefig(\"benchmark_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd905233-5b7e-44b4-ae60-c1e1f1044321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now requires a pass in of t as the number of threads (per block) and b as number of blocks (per grid)\n",
    "function apply_gate_gpu_64plus_threadchoice!(s, c, num_bits, t, b)\n",
    "    #z_start is where the z's start for example if there are 67 bits it will be 3, if there are 129 bits it will be 4 etc.\n",
    "    #x_stop is just 1 before z_start\n",
    "    function kernel(s, c, num_bits, numBitstrings, tempXZArr, x_stop, z_start)\n",
    "        i = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n",
    "        if i <= num_bits\n",
    "            for j in 1:64\n",
    "                #x's loop\n",
    "                for k in 1:x_stop\n",
    "                    if s[k, i] & 1 == 1\n",
    "                        for l in 1:numBitstrings\n",
    "                            #before it was just c[l, j], but now a bit more complicated indexing\n",
    "                            tempXZArr[i, l] ⊻= c[l, j+(k-1)*64]\n",
    "                        end\n",
    "                    end\n",
    "                    s[k, i] >>= 1\n",
    "                end\n",
    "                #z's loop\n",
    "                for k in z_start:numBitstrings\n",
    "                    if s[k, i] & 1 == 1\n",
    "                        for l in 1:numBitstrings\n",
    "                             #before it was just c[l, j+ num_bits], but now a bit more complicated\n",
    "                            tempXZArr[i, l] ⊻= c[l, j+num_bits+(k-z_start)*64]\n",
    "                        end\n",
    "                    end\n",
    "                    s[k, i] >>= 1\n",
    "                end\n",
    "            end\n",
    "            for a in 1:numBitstrings\n",
    "                s[a, i] = tempXZArr[i, a]\n",
    "            end\n",
    "        end\n",
    "        return \n",
    "    end\n",
    "\n",
    "\n",
    "    # Launch the kernel\n",
    "    #currently we are passed in number of bits, which I don't think is unreasonable, although could be done away with, with some calculations)\n",
    "    numBitstrings = ceil(Int, num_bits/64)*2\n",
    "    x_stop = Int(numBitstrings / 2)\n",
    "    z_start = Int((numBitstrings / 2) + 1)\n",
    "    tempXZArr = CUDA.fill(UInt64(0), num_bits, numBitstrings)\n",
    "    #starting XZArr at 0's\n",
    "    @cuda threads=t blocks=b kernel(s, c, num_bits, numBitstrings, tempXZArr, x_stop, z_start)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5cec8a-4fdc-4590-b02e-003aae2bdded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking apply_gate_gpu with varying GPU parameters\n",
    "num_qbits = 512\n",
    "cliff = random_clifford(num_qbits)\n",
    "c_gpu = CuArray(cliff.tab.xzs)  # Copy `c` to GPU memory once\n",
    "s = random_stabilizer(num_qbits)\n",
    "s_gpu = CuArray(s.tab.xzs)\n",
    "\n",
    "println(\"Benchmarking GPU: Threads per Block = 32, Blocks per Grid = 4\")\n",
    "# Define your kernel launch configuration\n",
    "gpu_time = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 32, 4)\n",
    "println(\"GPU Time: $gpu_time\")\n",
    "show(stdout, \"text/plain\", gpu_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a810a5e-ae03-4f8c-9fad-fea236629ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using Plots\n",
    "\n",
    "# Function to benchmark for a specific configuration\n",
    "function benchmark_gpu(num_qbits)\n",
    "    cliff = random_clifford(num_qbits)\n",
    "    c_gpu = CuArray(cliff.tab.xzs) \n",
    "    s = random_stabilizer(num_qbits)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "\n",
    "    # Explicit configurations for threads per block and blocks per grid\n",
    "    println(\"Benchmarking GPU: Threads per Block = 32, Blocks per Grid = 1\")\n",
    "    gpu_time_32_1 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 32, 1)\n",
    "    println(\"GPU Times: $gpu_time_32_1\")\n",
    "    median_time_32_1 = median(gpu_time_32_1)\n",
    "    println(\"Median Time: $median_time_32_1\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 32, Blocks per Grid = 2\")\n",
    "    gpu_time_32_2 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 32, 2)\n",
    "    println(\"GPU Times: $gpu_time_32_2\")\n",
    "    median_time_32_2 = median(gpu_time_32_2)\n",
    "    println(\"Median Time: $median_time_32_2\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 32, Blocks per Grid = 4\")\n",
    "    gpu_time_32_4 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 32, 4)\n",
    "    println(\"GPU Times: $gpu_time_32_4\")\n",
    "    median_time_32_4 = median(gpu_time_32_4)\n",
    "    println(\"Median Time: $median_time_32_4\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 32, Blocks per Grid = 8\")\n",
    "    gpu_time_32_8 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 32, 8)\n",
    "    println(\"GPU Times: $gpu_time_32_8\")\n",
    "    median_time_32_8 = median(gpu_time_32_8)\n",
    "    println(\"Median Time: $median_time_32_8\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 64, Blocks per Grid = 1\")\n",
    "    gpu_time_64_1 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 64, 1)\n",
    "    println(\"GPU Times: $gpu_time_64_1\")\n",
    "    median_time_64_1 = median(gpu_time_64_1)\n",
    "    println(\"Median Time: $median_time_64_1\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 64, Blocks per Grid = 2\")\n",
    "    gpu_time_64_2 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 64, 2)\n",
    "    println(\"GPU Times: $gpu_time_64_2\")\n",
    "    median_time_64_2 = median(gpu_time_64_2)\n",
    "    println(\"Median Time: $median_time_64_2\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 64, Blocks per Grid = 4\")\n",
    "    gpu_time_64_4 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 64, 4)\n",
    "    println(\"GPU Times: $gpu_time_64_4\")\n",
    "    median_time_64_4 = median(gpu_time_64_4)\n",
    "    println(\"Median Time: $median_time_64_4\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 64, Blocks per Grid = 8\")\n",
    "    gpu_time_64_8 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 64, 8)\n",
    "    println(\"GPU Times: $gpu_time_64_8\")\n",
    "    median_time_64_8 = median(gpu_time_64_8)\n",
    "    println(\"Median Time: $median_time_64_8\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 128, Blocks per Grid = 1\")\n",
    "    gpu_time_128_1 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 128, 1)\n",
    "    println(\"GPU Times: $gpu_time_128_1\")\n",
    "    median_time_128_1 = median(gpu_time_128_1)\n",
    "    println(\"Median Time: $median_time_128_1\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 128, Blocks per Grid = 2\")\n",
    "    gpu_time_128_2 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 128, 2)\n",
    "    println(\"GPU Times: $gpu_time_128_2\")\n",
    "    median_time_128_2 = median(gpu_time_128_2)\n",
    "    println(\"Median Time: $median_time_128_2\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 128, Blocks per Grid = 4\")\n",
    "    gpu_time_128_4 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 128, 4)\n",
    "    println(\"GPU Times: $gpu_time_128_4\")\n",
    "    median_time_128_4 = median(gpu_time_128_4)\n",
    "    println(\"Median Time: $median_time_128_4\")\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    println(\"Benchmarking GPU: Threads per Block = 128, Blocks per Grid = 8\")\n",
    "    gpu_time_128_8 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 128, 8)\n",
    "    println(\"GPU Times: $gpu_time_128_8\")\n",
    "    median_time_128_8 = median(gpu_time_128_8)\n",
    "    println(\"Median Time: $median_time_128_8\")\n",
    "\n",
    "    # Store all the results in a dictionary\n",
    "    results = Dict(\n",
    "        (32, 1) => (gpu_time_32_1, median_time_32_1), (32, 2) => (gpu_time_32_2, median_time_32_2), \n",
    "        (32, 4) => (gpu_time_32_4, median_time_32_4), (32, 8) => (gpu_time_32_8, median_time_32_8),\n",
    "        (64, 1) => (gpu_time_64_1, median_time_64_1), (64, 2) => (gpu_time_64_2, median_time_64_2), \n",
    "        (64, 4) => (gpu_time_64_4, median_time_64_4), (64, 8) => (gpu_time_64_8, median_time_64_8),\n",
    "        (128, 1) => (gpu_time_128_1, median_time_128_1), (128, 2) => (gpu_time_128_2, median_time_128_2), \n",
    "        (128, 4) => (gpu_time_128_4, median_time_128_4), (128, 8) => (gpu_time_128_8, median_time_128_8)\n",
    "    )\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "# Run the benchmark for a chosen number of qubits (e.g., 512)\n",
    "results = benchmark_gpu(512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ec78e-cad3-4fcf-8cdb-61a21b4f0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using BenchmarkTools\n",
    "using CUDA\n",
    "#set same num_qbits at the top as well, so doesn't get over-ridden by potential global variable\n",
    "num_qbits=512\n",
    "# Function to benchmark for a specific configuration\n",
    "function benchmark_gpu(num_qbits)\n",
    "    cliff = random_clifford(num_qbits)\n",
    "    c_gpu = CuArray(cliff.tab.xzs) \n",
    "    s = random_stabilizer(num_qbits)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    # Define an array to store the median times\n",
    "    median_times = Float64[]\n",
    "\n",
    "    # Benchmarking for Threads per Block = 32\n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_32_1 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 32, 1)\n",
    "    push!(median_times, median(gpu_time_32_1).time / 1e6)  # Convert to seconds\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_32_2 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 32, 2)\n",
    "    push!(median_times, median(gpu_time_32_2).time / 1e6)  # Convert to seconds\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_32_4 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 32, 4)\n",
    "    push!(median_times, median(gpu_time_32_4).time / 1e6)  # Convert to seconds\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_32_8 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 32, 8)\n",
    "    push!(median_times, median(gpu_time_32_8).time / 1e6)  # Convert to seconds\n",
    "\n",
    "    # Benchmarking for Threads per Block = 64\n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_64_1 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 64, 1)\n",
    "    push!(median_times, median(gpu_time_64_1).time / 1e6)  # Convert to seconds\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_64_2 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 64, 2)\n",
    "    push!(median_times, median(gpu_time_64_2).time / 1e6)  # Convert to seconds\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_64_4 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 64, 4)\n",
    "    push!(median_times, median(gpu_time_64_4).time / 1e6)  # Convert to seconds\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_64_8 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 64, 8)\n",
    "    push!(median_times, median(gpu_time_64_8).time / 1e6)  # Convert to seconds\n",
    "\n",
    "    # Benchmarking for Threads per Block = 128\n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_128_1 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 128, 1)\n",
    "    push!(median_times, median(gpu_time_128_1).time / 1e6)  # Convert to seconds\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_128_2 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 128, 2)\n",
    "    push!(median_times, median(gpu_time_128_2).time / 1e6)  # Convert to seconds\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_128_4 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 128, 4)\n",
    "    push!(median_times, median(gpu_time_128_4).time / 1e6)  # Convert to seconds\n",
    "    \n",
    "    c_gpu = CuArray(cliff.tab.xzs)\n",
    "    s_gpu = CuArray(s.tab.xzs)\n",
    "    gpu_time_128_8 = @benchmark CUDA.@sync apply_gate_gpu_64plus_threadchoice!(s_gpu, c_gpu, num_qbits, 128, 8)\n",
    "    push!(median_times, median(gpu_time_128_8).time / 1e6)  # Convert to seconds\n",
    "\n",
    "    return median_times\n",
    "end\n",
    "\n",
    "# Run the benchmark for a chosen number of qubits (e.g., 512)\n",
    "median_times = benchmark_gpu(512)\n",
    "\n",
    "# Prepare the data for plotting\n",
    "threads_per_block_values = [32, 64, 128]\n",
    "blocks_per_grid_values = [1, 2, 4, 8]\n",
    "\n",
    "# Reshape the data for plotting (grid rows for blocks_per_grid and columns for threads_per_block)\n",
    "plot_data = reshape(median_times, length(blocks_per_grid_values), length(threads_per_block_values))\n",
    "\n",
    "# Plotting the results\n",
    "p = heatmap(blocks_per_grid_values, threads_per_block_values, plot_data,\n",
    "            xlabel=\"Blocks per Grid\", ylabel=\"Threads per Block\",\n",
    "            title=\"GPU Time vs Threads/Blocks Configuration\", color=:viridis)\n",
    "\n",
    "# Show the plot\n",
    "display(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (8 threads) 1.10.1",
   "language": "julia",
   "name": "julia-_8-threads_-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
